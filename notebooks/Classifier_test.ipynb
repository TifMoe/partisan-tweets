{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T04:23:25.282646Z",
     "start_time": "2018-03-07T04:23:24.691623Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from configparser import ConfigParser\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    import spacy\n",
    "    nlp = spacy.load('en')\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "os.chdir('/Users/tmoeller/ds/twitter-project/partisan-tweets')\n",
    "import src.data.aws_ec2_functions as aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:11:40.671291Z",
     "start_time": "2018-03-07T03:11:40.273316Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure AWS Ec2 Instance is running and get public IP address\n",
    "instance = aws.fetch_instances()[0]\n",
    "\n",
    "if aws.instance_state(instance) != 'running':\n",
    "    print('Starting {} instance now'.format(instance.public_ip_address))\n",
    "    aws.start_instance(instance, safety=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:11:42.091399Z",
     "start_time": "2018-03-07T03:11:42.083825Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.ini']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuations = string.punctuation\n",
    "config = ConfigParser()\n",
    "config.read('config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:11:42.992877Z",
     "start_time": "2018-03-07T03:11:42.983433Z"
    }
   },
   "outputs": [],
   "source": [
    "client = MongoClient(\"mongodb://{}:{}@{}/{}\".format(\n",
    "                        config.get('MongoConfig', 'user'),\n",
    "                        config.get('MongoConfig', 'password'),\n",
    "                        instance.public_ip_address,\n",
    "                        config.get('MongoConfig', 'db'),\n",
    "                        int(config.get('MongoConfig', 'port'))))\n",
    "\n",
    "db = client.twitter_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:12:13.884669Z",
     "start_time": "2018-03-07T03:12:13.807490Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_train_test(train_group, test_group):\n",
    "    cursor = db.train_test_dict.find({'group': train_group}, { 'screen_names': 1, '_id': 0 })\n",
    "    train_group = [doc for doc in cursor]\n",
    "\n",
    "    cursor = db.train_test_dict.find({'group': test_group}, { 'screen_names': 1, '_id': 0 })\n",
    "    test_group = [doc for doc in cursor]\n",
    "\n",
    "    cursor = db.fav_tweets_dict.find( { \"name\": { \"$in\": train_group[0]['screen_names'] } } )\n",
    "    train = [doc for doc in cursor]\n",
    "\n",
    "    cursor = db.legislator_tweets_dict.find( { \"name\": { \"$in\": test_group[0]['screen_names'] } } )\n",
    "    test = [doc for doc in cursor]\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def unlist_tweets(list_tweet_dicts):\n",
    "    tweets_labels = []\n",
    "\n",
    "    for dict in list_tweet_dicts:\n",
    "        for tweet in dict['tweets']:\n",
    "            tweets_labels.append([tweet, dict['party']])\n",
    "\n",
    "    return tweets_labels\n",
    "\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Function to remove urls, numbers and punctuation, and make lowercase\n",
    "    \"\"\"\n",
    "    no_url = re.sub(r'http\\S+', '', tweet)\n",
    "    clean = re.sub(r'[^\\w\\s]', '', no_url)\n",
    "\n",
    "    result = ''.join([str(i).replace('\\n', ' ').lower() for i in clean if not i.isdigit()])\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def spacy_tokenizer(tweet):\n",
    "    \"\"\"\n",
    "    Utility function to remove stopwords, ignore pronouns and tokenize words before vectorizing\n",
    "    \"\"\"\n",
    "    doc = nlp(tweet)\n",
    "    tokens = [token.orth_ for token in doc if not token.is_stop]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:13:05.609892Z",
     "start_time": "2018-03-07T03:12:43.480896Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_a, test_b = fetch_train_test('train_a', 'test_b')\n",
    "\n",
    "train_data = unlist_tweets(train_a)\n",
    "test_data = unlist_tweets(test_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:41:18.833947Z",
     "start_time": "2018-03-07T03:41:18.828217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143531\n",
      "13610\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T04:25:06.524175Z",
     "start_time": "2018-03-07T04:24:58.308113Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_train = [clean_tweet(tweet[0]) for tweet in train_data]\n",
    "clean_test = [clean_tweet(tweet[0]) for tweet in test_data]\n",
    "\n",
    "y_train = [1 if tweet[1]=='R' else 0 for tweet in train_data]\n",
    "y_test = [1 if tweet[1]=='R' else 0 for tweet in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T04:25:46.297296Z",
     "start_time": "2018-03-07T04:25:46.282918Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 64598, 1: 78933})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:36:13.339483Z",
     "start_time": "2018-03-07T03:13:20.374016Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_train = [spacy_tokenizer(tweet) for tweet in clean_train]\n",
    "tokenized_test = [spacy_tokenizer(tweet) for tweet in clean_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:36:21.298891Z",
     "start_time": "2018-03-07T03:36:15.043862Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let X be a list of tokenized texts (i.e. list of lists of tokens)\n",
    "model = gensim.models.Word2Vec(tokenized_train, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:43:43.141234Z",
     "start_time": "2018-03-07T03:43:43.123878Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tmoeller/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:47:49.553480Z",
     "start_time": "2018-03-07T03:47:49.490720Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.items())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(word2vec.items())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of\n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T04:16:40.031862Z",
     "start_time": "2018-03-07T04:16:40.026479Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bern_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
    "bern_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:48:29.142975Z",
     "start_time": "2018-03-07T03:48:29.137988Z"
    }
   },
   "outputs": [],
   "source": [
    "etree_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T04:35:50.446169Z",
     "start_time": "2018-03-07T04:26:26.310617Z"
    }
   },
   "outputs": [],
   "source": [
    "all_models = [\n",
    "    (\"bern_nb\", bern_nb),\n",
    "    (\"bern_nb_tfidf\", bern_nb_tfidf),\n",
    "    (\"w2v\", etree_w2v),\n",
    "    (\"w2v_tfidf\", etree_w2v_tfidf)\n",
    "]\n",
    "\n",
    "\n",
    "unsorted_scores = [(name, cross_val_score(model, clean_train, y_train, cv=3, scoring='roc_auc').mean()) for name, model in all_models]\n",
    "scores = sorted(unsorted_scores, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T04:36:30.454554Z",
     "start_time": "2018-03-07T04:36:30.448941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model            score\n",
      "-------------  -------\n",
      "bern_nb         0.5414\n",
      "bern_nb_tfidf   0.5414\n",
      "w2v_tfidf       0.5328\n",
      "w2v             0.5309\n"
     ]
    }
   ],
   "source": [
    "print (tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
