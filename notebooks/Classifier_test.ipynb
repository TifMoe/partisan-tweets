{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T14:31:22.881544Z",
     "start_time": "2018-03-07T14:31:22.336512Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from configparser import ConfigParser\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    import spacy\n",
    "    nlp = spacy.load('en')\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "os.chdir('/Users/tmoeller/ds/twitter-project/partisan-tweets')\n",
    "import src.data.aws_ec2_functions as aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T12:03:43.934893Z",
     "start_time": "2018-03-07T12:03:43.517897Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure AWS Ec2 Instance is running and get public IP address\n",
    "instance = aws.fetch_instances()[0]\n",
    "\n",
    "if aws.instance_state(instance) != 'running':\n",
    "    print('Starting {} instance now'.format(instance.public_ip_address))\n",
    "    aws.start_instance(instance, safety=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:11:42.091399Z",
     "start_time": "2018-03-07T03:11:42.083825Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.ini']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuations = string.punctuation\n",
    "config = ConfigParser()\n",
    "config.read('config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T12:04:25.694440Z",
     "start_time": "2018-03-07T12:04:25.685703Z"
    }
   },
   "outputs": [],
   "source": [
    "client = MongoClient(\"mongodb://{}:{}@{}/{}\".format(\n",
    "                        config.get('MongoConfig', 'user'),\n",
    "                        config.get('MongoConfig', 'password'),\n",
    "                        instance.public_ip_address,\n",
    "                        config.get('MongoConfig', 'db'),\n",
    "                        int(config.get('MongoConfig', 'port'))))\n",
    "\n",
    "db = client.twitter_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T18:59:22.503747Z",
     "start_time": "2018-03-07T18:59:22.416448Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_train_test(train_group, test_group):\n",
    "    cursor = db.train_test_dict.find({'group': train_group}, { 'screen_names': 1, '_id': 0 })\n",
    "    train_group = [doc for doc in cursor]\n",
    "\n",
    "    cursor = db.train_test_dict.find({'group': test_group}, { 'screen_names': 1, '_id': 0 })\n",
    "    test_group = [doc for doc in cursor]\n",
    "\n",
    "    cursor = db.fav_tweets_dict.find( { \"name\": { \"$in\": train_group[0]['screen_names'] } } )\n",
    "    train = [doc for doc in cursor]\n",
    "\n",
    "    cursor = db.legislator_tweets_dict.find( { \"name\": { \"$in\": test_group[0]['screen_names'] } } )\n",
    "    test = [doc for doc in cursor]\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def unlist_tweets(list_tweet_dicts):\n",
    "    tweets_labels = []\n",
    "\n",
    "    for dict in list_tweet_dicts:\n",
    "        for tweet in dict['tweets']:\n",
    "            tweets_labels.append([tweet, dict['party']])\n",
    "\n",
    "    return tweets_labels\n",
    "\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Function to remove urls, numbers and punctuation, and make lowercase\n",
    "    \"\"\"\n",
    "    no_url = re.sub(r'http\\S+', '', tweet)\n",
    "    clean = re.sub(r'[^\\w\\s]', '', no_url)\n",
    "\n",
    "    result = ''.join([str(i).replace('\\n', ' ').lower() for i in clean if not i.isdigit()])\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def spacy_tokenizer(tweet):\n",
    "    \"\"\"\n",
    "    Utility function to remove stopwords, ignore pronouns and tokenize words before vectorizing\n",
    "    \"\"\"\n",
    "    doc = nlp(tweet)\n",
    "    tokens = [token.orth_ for token in doc if not token.is_stop]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T14:39:50.447532Z",
     "start_time": "2018-03-07T14:39:43.151468Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_a, test_b = fetch_train_test('train_a', 'test_b')\n",
    "\n",
    "train_data = unlist_tweets(train_a)\n",
    "test_data = unlist_tweets(test_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T14:40:44.843064Z",
     "start_time": "2018-03-07T14:40:44.838016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143531\n",
      "13610\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T14:41:46.160104Z",
     "start_time": "2018-03-07T14:41:38.277344Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_train = [clean_tweet(tweet[0]) for tweet in train_data]\n",
    "clean_test = [clean_tweet(tweet[0]) for tweet in test_data]\n",
    "\n",
    "y_train = [1 if tweet[1]=='R' else 0 for tweet in train_data]\n",
    "y_test = [1 if tweet[1]=='R' else 0 for tweet in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T04:25:46.297296Z",
     "start_time": "2018-03-07T04:25:46.282918Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 64598, 1: 78933})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:36:13.339483Z",
     "start_time": "2018-03-07T03:13:20.374016Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_train = [spacy_tokenizer(tweet) for tweet in clean_train]\n",
    "tokenized_test = [spacy_tokenizer(tweet) for tweet in clean_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T16:51:13.912724Z",
     "start_time": "2018-03-07T16:51:13.855897Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143531, 1)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.array(tokenized_train).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T17:02:23.743692Z",
     "start_time": "2018-03-07T17:02:16.264999Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tmoeller/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(tokenized_train, size=100, window=5, min_count=5, workers=2)\n",
    "w2v = {w: vec for w, vec in zip(model.wv.index2word, model.wv.syn0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T19:29:49.152753Z",
     "start_time": "2018-03-07T19:29:47.909454Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=<function <lambda> at 0x125c39950>, binary=False,\n",
       "        decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=None, smooth_idf=True, stop_words=None,\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "tfidf.fit(np.array(tokenized_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T19:30:42.737718Z",
     "start_time": "2018-03-07T19:30:42.724537Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_idf = max(tfidf.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T19:31:55.642263Z",
     "start_time": "2018-03-07T19:31:55.635829Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.18116610527752"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T19:33:21.735829Z",
     "start_time": "2018-03-07T19:33:21.730077Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110672"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T19:35:07.370719Z",
     "start_time": "2018-03-07T19:34:17.543570Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('carter', 8.086821543055418),\n",
       " ('film', 7.458212883633045),\n",
       " ('old', 6.361083174925158),\n",
       " ('crime', 6.990990897349186),\n",
       " ('drama', 8.763439421664154),\n",
       " ('starring', 9.783270832479149),\n",
       " ('michael', 7.0691783169209765),\n",
       " ('caine', 12.18116610527752),\n",
       " ('man', 5.165004952451596),\n",
       " ('returned', 8.715430202477794)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T19:44:40.731299Z",
     "start_time": "2018-03-07T19:44:40.722731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.65757465, -0.43149385, -0.173158  , -0.05396891,  0.9756762 ,\n",
       "       -0.56764764,  0.34622863, -0.9952162 ,  0.12716042, -0.6889521 ,\n",
       "       -0.36966366,  0.23572905, -0.1086284 , -0.02477333,  0.8455168 ,\n",
       "        0.5322241 ,  0.09200475, -0.14125866, -0.5532329 , -0.21689826,\n",
       "       -0.26331687, -0.20779441,  0.70469165,  0.43020454,  0.6197007 ,\n",
       "       -0.7655905 , -0.6719425 ,  0.3089493 ,  0.06360037,  0.4469036 ,\n",
       "       -0.2455574 , -0.19447382, -0.02673647, -0.17551348,  0.8663641 ,\n",
       "        0.5635876 ,  0.09866684, -0.41125426,  0.71162885, -0.5757415 ,\n",
       "       -0.37985703,  0.37381154, -0.26874182, -0.75298285, -0.45720792,\n",
       "       -0.24957158, -0.5306569 ,  0.15163025,  0.14710161, -0.12933388,\n",
       "       -0.17411111,  0.36777452,  0.8360838 , -0.50275666,  0.3028484 ,\n",
       "        0.2657561 ,  0.23719683,  1.0456861 , -0.4118023 , -0.5747189 ,\n",
       "       -1.2507479 , -0.6502701 ,  0.12593256, -0.45259246, -0.7551309 ,\n",
       "       -0.36980247,  0.67286927, -0.93774974, -0.08849384, -0.5362435 ,\n",
       "       -0.23705634, -1.9238791 ,  0.5495023 , -0.28160128,  0.65035325,\n",
       "        0.25871938,  0.14370506,  1.2492895 , -0.34960932,  0.22339533,\n",
       "       -0.05527705, -0.10239807, -0.844484  ,  1.0400658 , -0.03772491,\n",
       "       -0.03891698,  0.1485872 ,  0.57156116, -0.1517595 ,  0.18341176,\n",
       "        0.03373681,  0.21797296,  0.61125755, -0.10134026, -1.7384849 ,\n",
       "       -0.21517262, -0.48406047,  0.5776569 ,  0.89381146,  0.01474673],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['crime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T19:56:53.218192Z",
     "start_time": "2018-03-07T19:56:53.171155Z"
    }
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.values())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(word2vec.values())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of\n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T19:52:25.785491Z",
     "start_time": "2018-03-07T19:52:25.772710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carter', 'film', 'old', 'crime', 'drama', 'starring', 'michael', 'caine', 'man', 'returned', 'home', 'investigate'] \n",
      "\n",
      "[[ 0.4478975  -0.8819539   0.03672072 ...  1.2280468  -0.3594811\n",
      "   0.8130759 ]\n",
      " [ 0.01542316 -0.22520828  0.03162662 ...  2.3986938  -1.664752\n",
      "   0.13381721]\n",
      " [ 0.7504242  -0.8463364   0.08991772 ...  1.9839908  -0.37392592\n",
      "   0.17811705]\n",
      " ...\n",
      " [ 0.67234826 -0.79537976  0.15654385 ...  1.470461   -0.34150916\n",
      "   0.6871164 ]\n",
      " [ 0.4761072  -0.5334214  -0.00780101 ...  2.139413   -1.1525741\n",
      "   0.8540492 ]\n",
      " [ 0.70783883 -0.65458965 -0.31876636 ...  1.3232678  -0.19453858\n",
      "   0.64163744]]\n"
     ]
    }
   ],
   "source": [
    "test = tokenized_train[0]\n",
    "print(test, '\\n')\n",
    "\n",
    "tf_idf_test = TfidfEmbeddingVectorizer(w2v)\n",
    "test_fit = tf_idf_test.fit(test)\n",
    "test_trans = tf_idf_test.transform(test)\n",
    "print(test_trans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T19:53:28.057948Z",
     "start_time": "2018-03-07T19:53:28.046436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4478975 , -0.8819539 ,  0.03672072, ...,  1.2280468 ,\n",
       "        -0.3594811 ,  0.8130759 ],\n",
       "       [ 0.01542316, -0.22520828,  0.03162662, ...,  2.3986938 ,\n",
       "        -1.664752  ,  0.13381721],\n",
       "       [ 0.7504242 , -0.8463364 ,  0.08991772, ...,  1.9839908 ,\n",
       "        -0.37392592,  0.17811705],\n",
       "       ...,\n",
       "       [ 0.67234826, -0.79537976,  0.15654385, ...,  1.470461  ,\n",
       "        -0.34150916,  0.6871164 ],\n",
       "       [ 0.4761072 , -0.5334214 , -0.00780101, ...,  2.139413  ,\n",
       "        -1.1525741 ,  0.8540492 ],\n",
       "       [ 0.70783883, -0.65458965, -0.31876636, ...,  1.3232678 ,\n",
       "        -0.19453858,  0.64163744]], dtype=float32)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_test2 = TfidfEmbeddingVectorizer(w2v)\n",
    "embeddings = tf_idf_test2.fit(test).transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T19:59:59.138466Z",
     "start_time": "2018-03-07T19:59:59.109508Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [12, 143531]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-cdf395778c06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                 \u001b[0mtest_trans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                 cv=3, scoring='roc_auc')\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \"\"\"\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [12, 143531]"
     ]
    }
   ],
   "source": [
    "cross_val_score(ExtraTreesClassifier(n_estimators=200), \n",
    "                test_trans, \n",
    "                np.array(y_train), \n",
    "                cv=3, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T20:13:05.245071Z",
     "start_time": "2018-03-07T20:13:05.238495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 100)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(test_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T20:13:59.334949Z",
     "start_time": "2018-03-07T20:13:59.320626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143531,)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T16:57:01.235661Z",
     "start_time": "2018-03-07T16:57:01.230143Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multi_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), \n",
    "                     (\"multinomial nb\", MultinomialNB())])\n",
    "gaus_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), \n",
    "                          (\"gaussian nb\", GaussianNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T20:14:54.156562Z",
     "start_time": "2018-03-07T20:14:54.088643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                   Type                          Data/Info\n",
      "------------------------------------------------------------------\n",
      "BernoulliNB                ABCMeta                       <class 'sklearn.naive_bayes.BernoulliNB'>\n",
      "ConfigParser               ABCMeta                       <class 'configparser.ConfigParser'>\n",
      "CountVectorizer            type                          <class 'sklearn.feature_e<...>on.text.CountVectorizer'>\n",
      "Counter                    type                          <class 'collections.Counter'>\n",
      "ExtraTreesClassifier       ABCMeta                       <class 'sklearn.ensemble.<...>st.ExtraTreesClassifier'>\n",
      "GaussianNB                 ABCMeta                       <class 'sklearn.naive_bayes.GaussianNB'>\n",
      "MeanEmbeddingVectorizer    type                          <class '__main__.MeanEmbeddingVectorizer'>\n",
      "MongoClient                type                          <class 'pymongo.mongo_client.MongoClient'>\n",
      "MultinomialNB              ABCMeta                       <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "NamespaceMagics            MetaHasTraits                 <class 'IPython.core.magi<...>mespace.NamespaceMagics'>\n",
      "Pipeline                   ABCMeta                       <class 'sklearn.pipeline.Pipeline'>\n",
      "TfidfEmbeddingVectorizer   type                          <class '__main__.TfidfEmbeddingVectorizer'>\n",
      "TfidfVectorizer            type                          <class 'sklearn.feature_e<...>on.text.TfidfVectorizer'>\n",
      "all_models                 list                          n=3\n",
      "aws                        module                        <module 'src.data.aws_ec2<...>ta/aws_ec2_functions.py'>\n",
      "bern_nb                    Pipeline                      Pipeline(memory=None,\\n  <...>=None, fit_prior=True))])\n",
      "bern_nb_tfidf              Pipeline                      Pipeline(memory=None,\\n  <...>aussianNB(priors=None))])\n",
      "clean                      list                          n=29891\n",
      "clean_test                 list                          n=13610\n",
      "clean_train                list                          n=143531\n",
      "clean_tweet                function                      <function clean_tweet at 0x124a820d0>\n",
      "client                     MongoClient                   MongoClient(host=['18.191<...>ware=False, connect=True)\n",
      "config                     ConfigParser                  <configparser.ConfigParser object at 0x11572fc18>\n",
      "cross_val_score            function                      <function cross_val_score at 0x115595730>\n",
      "cursor                     Cursor                        <pymongo.cursor.Cursor object at 0x113c47588>\n",
      "data                       list                          n=29891\n",
      "data_a                     list                          n=0\n",
      "data_b                     list                          n=13610\n",
      "db                         Database                      Database(MongoClient(host<...>nect=True), 'twitter_db')\n",
      "defaultdict                type                          <class 'collections.defaultdict'>\n",
      "etree_w2v                  Pipeline                      Pipeline(memory=None,\\n  <...>e=0, warm_start=False))])\n",
      "etree_w2v_tfidf            Pipeline                      Pipeline(memory=None,\\n  <...>e=0, warm_start=False))])\n",
      "fetch_train_test           function                      <function fetch_train_test at 0x124a82400>\n",
      "gaus_nb_tfidf              Pipeline                      Pipeline(memory=None,\\n  <...>aussianNB(priors=None))])\n",
      "gensim                     module                        <module 'gensim' from '/U<...>ages/gensim/__init__.py'>\n",
      "get_ipython                function                      <function get_ipython at 0x10d21c620>\n",
      "getsizeof                  builtin_function_or_method    <built-in function getsizeof>\n",
      "instance                   ec2.Instance                  ec2.Instance(id='i-07f0fc75aa70cbb2f')\n",
      "json                       module                        <module 'json' from '/Use<...>hon3.6/json/__init__.py'>\n",
      "legislators                list                          n=515\n",
      "max_idf                    float64                       12.18116610527752\n",
      "model                      Pipeline                      Pipeline(memory=None,\\n  <...>e=0, warm_start=False))])\n",
      "model2                     Word2Vec                      Word2Vec(vocab=8807, size=100, alpha=0.025)\n",
      "mult_nb                    Pipeline                      Pipeline(memory=None,\\n  <...>=None, fit_prior=True))])\n",
      "multi_nb                   Pipeline                      Pipeline(memory=None,\\n  <...>=None, fit_prior=True))])\n",
      "name                       str                           w2v\n",
      "name_score                 tuple                         n=2\n",
      "nlp                        English                       <spacy.lang.en.English object at 0x11f8358d0>\n",
      "np                         module                        <module 'numpy' from '/Us<...>kages/numpy/__init__.py'>\n",
      "os                         module                        <module 'os' from '/Users<...>da3/lib/python3.6/os.py'>\n",
      "punctuations               str                           !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "re                         module                        <module 're' from '/Users<...>da3/lib/python3.6/re.py'>\n",
      "scores                     list                          n=3\n",
      "spacy                      module                        <module 'spacy' from '/Us<...>kages/spacy/__init__.py'>\n",
      "spacy_tokenizer            function                      <function spacy_tokenizer at 0x124a82488>\n",
      "string                     module                        <module 'string' from '/U<...>lib/python3.6/string.py'>\n",
      "tabulate                   function                      <function tabulate at 0x12da5a840>\n",
      "test                       list                          n=12\n",
      "test_a                     list                          n=0\n",
      "test_b                     list                          n=245\n",
      "test_data                  list                          n=13610\n",
      "test_fit                   TfidfEmbeddingVectorizer      <__main__.TfidfEmbeddingV<...>er object at 0x13e6bd5f8>\n",
      "test_trans                 ndarray                       12x100: 1200 elems, type `float32`, 4800 bytes\n",
      "tf_idf_test                TfidfEmbeddingVectorizer      <__main__.TfidfEmbeddingV<...>er object at 0x13e6bd5f8>\n",
      "tf_idf_test2               TfidfEmbeddingVectorizer      <__main__.TfidfEmbeddingV<...>er object at 0x13dfd2908>\n",
      "tfidf                      TfidfVectorizer               TfidfVectorizer(analyzer=<...>n        vocabulary=None)\n",
      "tokenized                  list                          n=29891\n",
      "tokenized_test             list                          n=13610\n",
      "tokenized_train            list                          n=143531\n",
      "train_a                    list                          n=2979\n",
      "train_data                 list                          n=143531\n",
      "unlist_tweets              function                      <function unlist_tweets at 0x124a82048>\n",
      "unsorted_scores            list                          n=1\n",
      "var_dic_list               function                      <function var_dic_list at 0x124a82598>\n",
      "w2v                        dict                          n=25252\n",
      "w2v_2                      dict                          n=8807\n",
      "warnings                   module                        <module 'warnings' from '<...>b/python3.6/warnings.py'>\n",
      "x                          str                           late night laundry and br<...>w cc masmartgrowth bosbrt\n",
      "y                          int                           0\n",
      "y_test                     list                          n=13610\n",
      "y_train                    list                          n=143531\n"
     ]
    }
   ],
   "source": [
    "whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T19:57:47.160415Z",
     "start_time": "2018-03-07T19:57:47.155717Z"
    }
   },
   "outputs": [],
   "source": [
    "etree_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-07T19:56:56.857Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_models = [\n",
    "    (\"multi_nb\", multi_nb),\n",
    "    (\"w2v\", etree_w2v),\n",
    "    (\"w2v_tfidf\", etree_w2v_tfidf)\n",
    "]\n",
    "\n",
    "unsorted_scores = []\n",
    "for name, model in all_models:\n",
    "    name_score = (name, cross_val_score(model, \n",
    "                                        np.array(tokenized_train), \n",
    "                                        np.array(y_train), \n",
    "                                        cv=3, scoring='roc_auc').mean())\n",
    "    print(name_score)\n",
    "    unsorted_scores.append(name_score)\n",
    "\n",
    "scores = sorted(unsorted_scores, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T16:00:29.031816Z",
     "start_time": "2018-03-07T16:00:24.401Z"
    }
   },
   "outputs": [],
   "source": [
    "print (tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test models on Legislator data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T12:12:21.469056Z",
     "start_time": "2018-03-07T12:12:17.976247Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor = db.legislator_tweets_dict.find( {} )\n",
    "legislators = [doc for doc in cursor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T12:13:03.058758Z",
     "start_time": "2018-03-07T12:13:03.040732Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = unlist_tweets(legislators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T12:13:43.278189Z",
     "start_time": "2018-03-07T12:13:43.273068Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29891"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T12:35:45.681101Z",
     "start_time": "2018-03-07T12:35:43.493659Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean = [clean_tweet(tweet[0]) for tweet in data]\n",
    "y = [1 if tweet[1]=='R' else 0 for tweet in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T12:15:05.608879Z",
     "start_time": "2018-03-07T12:15:05.601408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['talofa american samoas cadca group stopped by our dc office to talk about drugfree communities and see the capitol thanks and keep up the good work ',\n",
       " 'talofa today my colleagues and i in the housevetaffairs held an oversight hearing examining how recent legislation passed by congress has been implemented by the va read my pr here ',\n",
       " 'talofa im pleased to announce that the epa has awarded american samoa a grant for diesel emissions reduction this is the second grant that we have received from them in two weeks read my pr here  ',\n",
       " 'as chair of the health amp technology subcommittee cyber security is one of the most important issues facing our small businesses i remain committed in ensuring that our small businesses have the resources they need to protect themselves from cyber attacks ',\n",
       " 'before your weekend begins check out my top photos from this week in indiana   ',\n",
       " 'important information for anyone impacted by flooding follow idhs for more updates ',\n",
       " 'visited mckinney farms in kempton today where i spoke with hoosier farmers amp students from tipton high school about investing in agriculture jobs amp other important ag issues ',\n",
       " 'thank you for hosting me enjoyed hearing about how your facility is training our hoosier workers ',\n",
       " 'held my rd fairshotagenda roundtable at fwurbanleague from affordable housing and transportation to job training and treatment programs these talks have shed light on some issues we must address in order to ensure a fair shot for all hoosiers  ',\n",
       " 'visited clay hill ranch in laporte this morning where i made a few new friends amp learned about issues impacting our farmers including excessive regulations amp the need for rural broadband ']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(clean))\n",
    "clean[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T12:20:31.151452Z",
     "start_time": "2018-03-07T12:15:45.457270Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized = [spacy_tokenizer(tweet) for tweet in clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T12:21:13.297164Z",
     "start_time": "2018-03-07T12:21:11.645710Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tmoeller/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model2 = gensim.models.Word2Vec(tokenized, size=100)\n",
    "w2v_2 = dict(zip(model2.wv.index2word, model2.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T15:56:04.464730Z",
     "start_time": "2018-03-07T15:56:04.456354Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mult_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), \n",
    "                    (\"bernoulli nb\", MultinomialNB())])\n",
    "gaus_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), \n",
    "                          (\"bernoulli nb\", GaussianNB())])\n",
    "\n",
    "etree_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v_2)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v_2)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T15:59:26.644079Z",
     "start_time": "2018-03-07T15:59:26.614026Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(0) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-365939095355>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0munsorted_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_models\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munsorted_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-110-365939095355>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0munsorted_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_models\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munsorted_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \"\"\"\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \"\"\"\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \"\"\"\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[0;32m--> 119\u001b[0;31m                             \" a valid collection.\" % x)\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Singleton array array(0) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "all_models = [\n",
    "    (\"mult_nb\", mult_nb),\n",
    "    (\"w2v\", etree_w2v),\n",
    "    (\"w2v_tfidf\", etree_w2v_tfidf)\n",
    "]\n",
    "\n",
    "\n",
    "unsorted_scores = [(name, cross_val_score(model, tokenized, y, cv=3, scoring='roc_auc').mean()) for name, model in all_models]\n",
    "scores = sorted(unsorted_scores, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T15:56:57.591784Z",
     "start_time": "2018-03-07T15:55:30.641Z"
    }
   },
   "outputs": [],
   "source": [
    "print (tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
